import sys
import os

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c g·ªëc c·ªßa project
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.vector_stores.weaviate import WeaviateVectorStore
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from llama_index.llms.google_genai import GoogleGenAI
import weaviate
import google.generativeai as genai
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import t·ª´ th∆∞ m·ª•c src
from src.global_setting import WEAVIATE_URL, WEAVIATE_CLASS_NAME, INDEX_STORAGE

# C·∫•u h√¨nh API key t·ª´ environment variable
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    raise ValueError("GOOGLE_API_KEY not found in environment variables. Please check your .env file.")

genai.configure(api_key=google_api_key)

def query_vector_database(query_text: str, top_k: int = 5, use_cached_index: bool = True):
    """Query vector database t·ª´ Weaviate v·ªõi t√πy ch·ªçn s·ª≠ d·ª•ng cached index"""
    
    try:
        # 1. K·∫øt n·ªëi Weaviate v·ªõi client v4
        weaviate_client = weaviate.connect_to_local(host="localhost", port=8080)
        
        if not weaviate_client.is_ready():
            raise Exception("Weaviate is not ready")
        
        print(f"‚úì Connected to Weaviate at {WEAVIATE_URL}")
        
        # 2. Setup embedding model - s·ª≠ d·ª•ng GoogleGenAI
        google_embedding = GoogleGenAIEmbedding(
            model_name="models/embedding-001",
            api_key=google_api_key
        )
        
        # 3. Setup LLM cho query engine
        google_llm = GoogleGenAI(
            model="models/gemini-1.5-flash",
            api_key=google_api_key,
            temperature=0.1
        )
        
        # 4. T·ªëi ∆∞u: S·ª≠ d·ª•ng cached index n·∫øu c√≥
        if use_cached_index:
            index_storage_path = os.path.join(project_root, INDEX_STORAGE)
            if os.path.exists(index_storage_path) and os.listdir(index_storage_path):
                print(f"üöÄ Loading cached index from: {index_storage_path}")
                
                # Load t·ª´ cached storage (kh√¥ng c·∫ßn vector store khi load t·ª´ disk)
                storage_context = StorageContext.from_defaults(persist_dir=index_storage_path)
                
                index = VectorStoreIndex.from_storage_context(
                    storage_context=storage_context,
                    embed_model=google_embedding
                )
                print("‚úÖ Successfully loaded cached index!")
            else:
                print("‚ö†Ô∏è No cached index found, rebuilding from Weaviate...")
                use_cached_index = False
        
        # 5. Fallback: Rebuild index t·ª´ Weaviate (ch·∫≠m h∆°n)
        if not use_cached_index:
            print("üîÑ Rebuilding index from Weaviate vector store...")
            vector_store = WeaviateVectorStore(
                weaviate_client=weaviate_client,
                index_name=WEAVIATE_CLASS_NAME,
                text_key="content"
            )
            
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            index = VectorStoreIndex.from_vector_store(
                vector_store=vector_store,
                embed_model=google_embedding
            )
        
        # 6. T·∫°o query engine v·ªõi LLM v√† th·ª±c hi·ªán truy v·∫•n
        query_engine = index.as_query_engine(
            similarity_top_k=top_k,
            llm=google_llm
        )
        
        print(f"üîç Querying: {query_text}")
        print(f"ü§ñ Using LLM: {google_llm.model} (Gemini-1.5-Flash)")
        print(f"üìä Retrieving top {top_k} similar chunks from Weaviate...")
        
        # ƒê√¢y l√† n∆°i LLM ƒë∆∞·ª£c g·ªçi ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ retrieved context
        response = query_engine.query(query_text)
        
        print(f"\nüìã Answer (Generated by LLM): {response.response}")
        print(f"\nüìö Sources ({len(response.source_nodes)} results):")
        
        for i, node in enumerate(response.source_nodes, 1):
            print(f"\n{i}. Score: {node.score:.4f}")
            print(f"   Content: {node.text[:200]}...")
            if hasattr(node, 'metadata') and node.metadata:
                print(f"   Source: {node.metadata.get('filename', 'Unknown')}")
        
        # ƒê√≥ng k·∫øt n·ªëi
        weaviate_client.close()
        
        return response
    
    except Exception as e:
        print(f"‚ùå Error querying vector database: {e}")
        import traceback
        traceback.print_exc()
        return None

def build_and_cache_index():
    """Build index m·ªôt l·∫ßn v√† cache ƒë·ªÉ s·ª≠ d·ª•ng sau"""
    try:
        print("üîß Building and caching index for faster queries...")
        
        # K·∫øt n·ªëi Weaviate
        weaviate_client = weaviate.connect_to_local(host="localhost", port=8080)
        
        if not weaviate_client.is_ready():
            raise Exception("Weaviate is not ready")
        
        # Setup embedding
        google_embedding = GoogleGenAIEmbedding(
            model_name="models/embedding-001",
            api_key=google_api_key
        )
        
        # Setup vector store
        vector_store = WeaviateVectorStore(
            weaviate_client=weaviate_client,
            index_name=WEAVIATE_CLASS_NAME,
            text_key="content"
        )
        
        # Build index
        print("üìä Building index from Weaviate...")
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
            embed_model=google_embedding
        )
        
        # Cache index
        index_storage_path = os.path.join(project_root, INDEX_STORAGE)
        os.makedirs(index_storage_path, exist_ok=True)
        index.storage_context.persist(persist_dir=index_storage_path)
        
        weaviate_client.close()
        
        print(f"‚úÖ Index cached successfully at: {index_storage_path}")
        print("üöÄ Future queries will be much faster!")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error building index: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_weaviate_status():
    """Ki·ªÉm tra tr·∫°ng th√°i Weaviate v√† s·ªë l∆∞·ª£ng documents"""
    try:
        # S·ª≠ d·ª•ng Weaviate client v4
        client = weaviate.connect_to_local(host="localhost", port=8080)
        
        if client.is_ready():
            print(f"‚úÖ Weaviate is running at {WEAVIATE_URL}")
            
            # Ki·ªÉm tra collections (v4 s·ª≠ d·ª•ng collections thay v√¨ classes)
            collections = client.collections.list_all()
            collection_names = [col.name for col in collections.values()]
            print(f"üìã Available collections: {collection_names}")
            
            # ƒê·∫øm objects trong collection c·ªßa ch√∫ng ta
            if WEAVIATE_CLASS_NAME in collection_names:
                collection = client.collections.get(WEAVIATE_CLASS_NAME)
                count = len(list(collection.iterator()))
                print(f"üìä Objects in {WEAVIATE_CLASS_NAME}: {count}")
            else:
                print(f"‚ö†Ô∏è Collection {WEAVIATE_CLASS_NAME} not found")
                
            client.close()
        else:
            print("‚ùå Weaviate is not ready")
            
    except Exception as e:
        print(f"‚ùå Error connecting to Weaviate: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("üîç RAG Legal Consult System - Query Interface")
    print("=" * 50)
    
    # Ki·ªÉm tra tr·∫°ng th√°i Weaviate
    check_weaviate_status()
    
    # Ki·ªÉm tra xem c√≥ cached index kh√¥ng
    index_storage_path = os.path.join(project_root, INDEX_STORAGE)
    has_cached_index = os.path.exists(index_storage_path) and os.listdir(index_storage_path)
    
    if has_cached_index:
        print("‚úÖ Cached index found - queries will be fast!")
    else:
        print("‚ö†Ô∏è No cached index found")
        build_choice = input("üîß Build and cache index now for faster queries? (y/n): ").strip().lower()
        if build_choice in ['y', 'yes']:
            if build_and_cache_index():
                print("üéâ Index cached successfully!")
            else:
                print("‚ùå Failed to cache index, will rebuild on each query")
    
    print("\n" + "=" * 50)
    
    # M·ªôt s·ªë c√¢u h·ªèi m·∫´u
    sample_queries = [
        "ƒê·ªô tu·ªïi lao ƒë·ªông t·ªëi thi·ªÉu l√† bao nhi√™u?",
        "Quy ƒë·ªãnh v·ªÅ th·ªùi gian l√†m vi·ªác",
        "Ngh·ªâ ph√©p nƒÉm ƒë∆∞·ª£c bao nhi√™u ng√†y?",
        "Quy·ªÅn v√† nghƒ©a v·ª• c·ªßa ng∆∞·ªùi lao ƒë·ªông"
    ]
    
    print("üéØ Sample queries:")
    for i, query in enumerate(sample_queries, 1):
        print(f"{i}. {query}")
    
    print("\nüí° Tips:")
    print("- First query might be slower (building connections)")
    print("- Subsequent queries will be faster with cached index")
    print("- Type 'quit' to exit")
    
    print("\n" + "=" * 50)
    
    # Interactive query
    while True:
        try:
            query = input("\nüí¨ Enter your question (or 'quit' to exit): ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                print("üëã Goodbye!")
                break
            
            if not query:
                continue
            
            # Th·ª±c hi·ªán truy v·∫•n
            response = query_vector_database(query)
            
            print("\n" + "-" * 50)
            
        except KeyboardInterrupt:
            print("\nüëã Goodbye!")
            break
