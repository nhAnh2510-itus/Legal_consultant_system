import sys
import os

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c g·ªëc c·ªßa project
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.vector_stores.weaviate import WeaviateVectorStore
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from llama_index.llms.google_genai import GoogleGenAI
import weaviate
import google.generativeai as genai
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import t·ª´ th∆∞ m·ª•c src
from src.global_setting import WEAVIATE_URL, WEAVIATE_CLASS_NAME

# C·∫•u h√¨nh API key t·ª´ environment variable
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    raise ValueError("GOOGLE_API_KEY not found in environment variables. Please check your .env file.")

genai.configure(api_key=google_api_key)

def query_vector_database(query_text: str, top_k: int = 5):
    """Query vector database t·ª´ Weaviate"""
    
    try:
        # 1. K·∫øt n·ªëi Weaviate v·ªõi client v4
        weaviate_client = weaviate.connect_to_local(host="localhost", port=8080)
        
        if not weaviate_client.is_ready():
            raise Exception("Weaviate is not ready")
        
        print(f"‚úì Connected to Weaviate at {WEAVIATE_URL}")
        
        # 2. Setup embedding model - s·ª≠ d·ª•ng GoogleGenAI
        google_embedding = GoogleGenAIEmbedding(
            model_name="models/embedding-001",
            api_key=google_api_key
        )
        
        # 3. Setup LLM cho query engine
        google_llm = GoogleGenAI(
            model="models/gemini-1.5-flash",
            api_key=google_api_key,
            temperature=0.1
        )
        
        # 4. Build index t·ª´ Weaviate (Weaviate t·ª± qu·∫£n l√Ω index)
        print("üîÑ Connecting to Weaviate vector store...")
        vector_store = WeaviateVectorStore(
            weaviate_client=weaviate_client,
            index_name=WEAVIATE_CLASS_NAME,
            text_key="content"
        )
        
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
            embed_model=google_embedding
        )
        
        # 6. T·∫°o query engine v·ªõi LLM v√† th·ª±c hi·ªán truy v·∫•n
        query_engine = index.as_query_engine(
            similarity_top_k=top_k,
            llm=google_llm
        )
        
        print(f"üîç Querying: {query_text}")
        print(f"ü§ñ Using LLM: {google_llm.model} (Gemini-1.5-Flash)")
        print(f"üìä Retrieving top {top_k} similar chunks from Weaviate...")
        
        # ƒê√¢y l√† n∆°i LLM ƒë∆∞·ª£c g·ªçi ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ retrieved context
        response = query_engine.query(query_text)
        
        print(f"\nüìã Answer (Generated by LLM): {response.response}")
        print(f"\nüìö Sources ({len(response.source_nodes)} results):")
        
        for i, node in enumerate(response.source_nodes, 1):
            print(f"\n{i}. Score: {node.score:.4f}")
            print(f"   Content: {node.text[:200]}...")
            if hasattr(node, 'metadata') and node.metadata:
                print(f"   Source: {node.metadata.get('filename', 'Unknown')}")
        
        # ƒê√≥ng k·∫øt n·ªëi
        weaviate_client.close()
        
        return response
    
    except Exception as e:
        print(f"‚ùå Error querying vector database: {e}")
        import traceback
        traceback.print_exc()
        return None

def query_hybrid_search(query_text: str, top_k: int = 5, alpha: float = 0.6):
    """
    Th·ª±c hi·ªán hybrid search tr·ª±c ti·∫øp v·ªõi Weaviate
    alpha = 0.0: ch·ªâ BM25 (keyword search) 
    alpha = 1.0: ch·ªâ vector search (semantic)
    alpha = 0.7: k·∫øt h·ª£p 70% vector + 30% BM25 (recommended)
    """
    try:
        # 1. K·∫øt n·ªëi Weaviate
        weaviate_client = weaviate.connect_to_local(host="localhost", port=8080)
        
        if not weaviate_client.is_ready():
            raise Exception("Weaviate is not ready")
        
        print(f"‚úì Connected to Weaviate for hybrid search")
        
        # 2. Setup embedding model
        google_embedding = GoogleGenAIEmbedding(
            model_name="models/embedding-001",
            api_key=google_api_key
        )
        
        # 3. Setup LLM
        google_llm = GoogleGenAI(
            model="models/gemini-1.5-flash",
            api_key=google_api_key,
            temperature=0.1
        )
        
        # 4. T·∫°o query vector
        print("üîÑ Creating query vector...")
        query_vector = google_embedding.get_query_embedding(query_text)
        
        # 5. Th·ª±c hi·ªán hybrid search tr·ª±c ti·∫øp v·ªõi Weaviate
        collection = weaviate_client.collections.get(WEAVIATE_CLASS_NAME)
        
        print(f"üîç Hybrid Search: {query_text}")
        print(f"‚öñÔ∏è Alpha: {alpha} (Vector: {alpha*100:.0f}%, BM25: {(1-alpha)*100:.0f}%)")
        
        # Hybrid search query
        search_results = collection.query.hybrid(
            query=query_text,  # Text query cho BM25
            vector=query_vector,  # Vector query cho semantic search
            alpha=alpha,  # T·ª∑ l·ªá k·∫øt h·ª£p
            limit=top_k
            # B·ªè return_metadata v√¨ c√≥ th·ªÉ g√¢y l·ªói tham s·ªë
        )
        
        print(f"üìä Found {len(search_results.objects)} results")
        
        # 6. T·∫°o context t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm
        contexts = []
        sources_info = []
        
        for i, obj in enumerate(search_results.objects, 1):
            content = obj.properties.get('content', '')
            filename = obj.properties.get('filename', 'Unknown')
            # S·ª≠a c√°ch l·∫•y score ƒë·ªÉ tr√°nh l·ªói
            try:
                score = obj.metadata.score if hasattr(obj.metadata, 'score') and obj.metadata.score else 0.0
            except:
                score = 0.0
            
            contexts.append(content)
            sources_info.append({
                'content': content,
                'filename': filename,
                'score': score,
                'rank': i
            })
            
        #     print(f"\n{i}. Score: {score:.4f}")
        #     print(f"   Content: {content[:200]}...")
        #     print(f"   Source: {filename}")
        
        # 7. T·∫°o prompt cho LLM v·ªõi context
        if contexts:
            combined_context = "\n\n".join(contexts)
            
            prompt = f"""D·ª±a tr√™n th√¥ng tin sau ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† chi ti·∫øt:

Context:
{combined_context}

C√¢u h·ªèi: {query_text}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p. N·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan, h√£y n√≥i r√µ l√† kh√¥ng c√≥ th√¥ng tin."""
            
            print(f"\nü§ñ Generating answer with LLM...")
            llm_response = google_llm.complete(prompt)
            
            # print(f"\nüìã Hybrid Search Answer: {llm_response.text}")
            
            # ƒê√≥ng k·∫øt n·ªëi
            weaviate_client.close()
            
            return {
                'answer': llm_response.text,
                'sources': sources_info,
                'search_type': 'hybrid',
                'alpha': alpha
            }
        else:
            print("‚ùå No relevant results found")
            weaviate_client.close()
            return None
            
    except Exception as e:
        print(f"‚ùå Error in hybrid search: {e}")
        import traceback
        traceback.print_exc()
        return None

def check_weaviate_status():
    """Ki·ªÉm tra tr·∫°ng th√°i Weaviate v√† s·ªë l∆∞·ª£ng documents"""
    try:
        # S·ª≠ d·ª•ng Weaviate client v4
        client = weaviate.connect_to_local(host="localhost", port=8080)
        
        if client.is_ready():
            print(f"‚úÖ Weaviate is running at {WEAVIATE_URL}")
            
            # Ki·ªÉm tra collections (v4 s·ª≠ d·ª•ng collections thay v√¨ classes)
            collections = client.collections.list_all()
            collection_names = [col.name for col in collections.values()]
            print(f"üìã Available collections: {collection_names}")
            
            # ƒê·∫øm objects trong collection c·ªßa ch√∫ng ta
            if WEAVIATE_CLASS_NAME in collection_names:
                collection = client.collections.get(WEAVIATE_CLASS_NAME)
                count = len(list(collection.iterator()))
                print(f"üìä Objects in {WEAVIATE_CLASS_NAME}: {count}")
            else:
                print(f"‚ö†Ô∏è Collection {WEAVIATE_CLASS_NAME} not found")
                
            client.close()
        else:
            print("‚ùå Weaviate is not ready")
            
    except Exception as e:
        print(f"‚ùå Error connecting to Weaviate: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("üîç RAG Legal Consult System - Query Interface")
    print("=" * 50)
    
    # Ki·ªÉm tra tr·∫°ng th√°i Weaviate
    check_weaviate_status()
    
    print("\n" + "=" * 50)
    
    # M·ªôt s·ªë c√¢u h·ªèi m·∫´u
    sample_queries = [
        "ƒê·ªô tu·ªïi lao ƒë·ªông t·ªëi thi·ªÉu l√† bao nhi√™u?",
        "Quy ƒë·ªãnh v·ªÅ th·ªùi gian l√†m vi·ªác",
        "Ngh·ªâ ph√©p nƒÉm ƒë∆∞·ª£c bao nhi√™u ng√†y?",
        "Quy·ªÅn v√† nghƒ©a v·ª• c·ªßa ng∆∞·ªùi lao ƒë·ªông"
    ]
    
    print("üéØ Sample queries:")
    for i, query in enumerate(sample_queries, 1):
        print(f"{i}. {query}")
    
    print("\nüí° Search Options:")
    print("- 's' or 'semantic': Semantic search only (vector similarity)")
    print("- 'h' or 'hybrid': Hybrid search (vector + keyword/BM25)")
    print("- Hybrid search is recommended for better accuracy!")
    print("- Type 'quit' to exit")
    
    print("\n" + "=" * 50)
    
    # Interactive query
    while True:
        try:
            print("\nÔøΩ Choose search method:")
            print("1. Semantic Search (vector only)")
            print("2. Hybrid Search (vector + BM25) - Recommended")
            
            search_choice = input("Select option (1/2) or enter question directly: ").strip()
            
            # N·∫øu user nh·∫≠p s·ªë, ch·ªçn search method
            if search_choice in ['1', '2']:
                query = input("\nüí¨ Enter your question: ").strip()
                
                if query.lower() in ['quit', 'exit', 'q']:
                    print("üëã Goodbye!")
                    break
                
                if not query:
                    continue
                
                if search_choice == '1':
                    print("\nüîç Using Semantic Search...")
                    response = query_vector_database(query)
                else:
                    print("\nüîç Using Hybrid Search...")
                    response = query_hybrid_search(query, alpha=0.7)
                    
            # N·∫øu user nh·∫≠p tr·ª±c ti·∫øp c√¢u h·ªèi, d√πng hybrid search m·∫∑c ƒë·ªãnh
            elif search_choice and search_choice.lower() not in ['quit', 'exit', 'q']:
                query = search_choice
                print("\nüîç Using Hybrid Search (default)...")
                response = query_hybrid_search(query, alpha=0.7)
            elif search_choice.lower() in ['quit', 'exit', 'q']:
                print("üëã Goodbye!")
                break
            else:
                continue
            
            print("\n" + "-" * 50)
            
        except KeyboardInterrupt:
            print("\nüëã Goodbye!")
            break
