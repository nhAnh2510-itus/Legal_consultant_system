import sys
import os

# ThÃªm Ä‘Æ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c gá»‘c cá»§a project
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.vector_stores.weaviate import WeaviateVectorStore
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from llama_index.llms.google_genai import GoogleGenAI
import weaviate
import google.generativeai as genai
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import tá»« thÆ° má»¥c src
from src.global_setting import WEAVIATE_URL, WEAVIATE_CLASS_NAME

# Cáº¥u hÃ¬nh API key tá»« environment variable
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    raise ValueError("GOOGLE_API_KEY not found in environment variables. Please check your .env file.")

genai.configure(api_key=google_api_key)

def query_vector_database(query_text: str, top_k: int = 5):
    """Query vector database tá»« Weaviate"""
    
    try:
        # 1. Káº¿t ná»‘i Weaviate vá»›i client v4
        weaviate_client = weaviate.connect_to_local(host="localhost", port=8080)
        
        if not weaviate_client.is_ready():
            raise Exception("Weaviate is not ready")
        
        print(f"âœ“ Connected to Weaviate at {WEAVIATE_URL}")
        
        # 2. Setup embedding model - sá»­ dá»¥ng GoogleGenAI
        google_embedding = GoogleGenAIEmbedding(
            model_name="models/embedding-001",
            api_key=google_api_key
        )
        
        # 3. Setup LLM cho query engine
        google_llm = GoogleGenAI(
            model="models/gemini-1.5-flash",
            api_key=google_api_key,
            temperature=0.1
        )
        
        # 4. Build index tá»« Weaviate (Weaviate tá»± quáº£n lÃ½ index)
        print("ğŸ”„ Connecting to Weaviate vector store...")
        vector_store = WeaviateVectorStore(
            weaviate_client=weaviate_client,
            index_name=WEAVIATE_CLASS_NAME,
            text_key="content"
        )
        
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
            embed_model=google_embedding
        )
        
        # 6. Táº¡o query engine vá»›i LLM vÃ  thá»±c hiá»‡n truy váº¥n
        query_engine = index.as_query_engine(
            similarity_top_k=top_k,
            llm=google_llm
        )
        
        print(f"ğŸ” Querying: {query_text}")
        print(f"ğŸ¤– Using LLM: {google_llm.model} (Gemini-1.5-Flash)")
        print(f"ğŸ“Š Retrieving top {top_k} similar chunks from Weaviate...")
        
        # ÄÃ¢y lÃ  nÆ¡i LLM Ä‘Æ°á»£c gá»i Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i tá»« retrieved context
        response = query_engine.query(query_text)
        
        print(f"\nğŸ“‹ Answer (Generated by LLM): {response.response}")
        print(f"\nğŸ“š Sources ({len(response.source_nodes)} results):")
        
        for i, node in enumerate(response.source_nodes, 1):
            print(f"\n{i}. Score: {node.score:.4f}")
            print(f"   Content: {node.text[:200]}...")
            if hasattr(node, 'metadata') and node.metadata:
                print(f"   Source: {node.metadata.get('filename', 'Unknown')}")
        
        # ÄÃ³ng káº¿t ná»‘i
        weaviate_client.close()
        
        return response
    
    except Exception as e:
        print(f"âŒ Error querying vector database: {e}")
        import traceback
        traceback.print_exc()
        return None

def check_weaviate_status():
    """Kiá»ƒm tra tráº¡ng thÃ¡i Weaviate vÃ  sá»‘ lÆ°á»£ng documents"""
    try:
        # Sá»­ dá»¥ng Weaviate client v4
        client = weaviate.connect_to_local(host="localhost", port=8080)
        
        if client.is_ready():
            print(f"âœ… Weaviate is running at {WEAVIATE_URL}")
            
            # Kiá»ƒm tra collections (v4 sá»­ dá»¥ng collections thay vÃ¬ classes)
            collections = client.collections.list_all()
            collection_names = [col.name for col in collections.values()]
            print(f"ğŸ“‹ Available collections: {collection_names}")
            
            # Äáº¿m objects trong collection cá»§a chÃºng ta
            if WEAVIATE_CLASS_NAME in collection_names:
                collection = client.collections.get(WEAVIATE_CLASS_NAME)
                count = len(list(collection.iterator()))
                print(f"ğŸ“Š Objects in {WEAVIATE_CLASS_NAME}: {count}")
            else:
                print(f"âš ï¸ Collection {WEAVIATE_CLASS_NAME} not found")
                
            client.close()
        else:
            print("âŒ Weaviate is not ready")
            
    except Exception as e:
        print(f"âŒ Error connecting to Weaviate: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸ” RAG Legal Consult System - Query Interface")
    print("=" * 50)
    
    # Kiá»ƒm tra tráº¡ng thÃ¡i Weaviate
    check_weaviate_status()
    
    print("\n" + "=" * 50)
    
    # Má»™t sá»‘ cÃ¢u há»i máº«u
    sample_queries = [
        "Äá»™ tuá»•i lao Ä‘á»™ng tá»‘i thiá»ƒu lÃ  bao nhiÃªu?",
        "Quy Ä‘á»‹nh vá» thá»i gian lÃ m viá»‡c",
        "Nghá»‰ phÃ©p nÄƒm Ä‘Æ°á»£c bao nhiÃªu ngÃ y?",
        "Quyá»n vÃ  nghÄ©a vá»¥ cá»§a ngÆ°á»i lao Ä‘á»™ng"
    ]
    
    print("ğŸ¯ Sample queries:")
    for i, query in enumerate(sample_queries, 1):
        print(f"{i}. {query}")
    
    print("\nğŸ’¡ Tips:")
    print("- Weaviate automatically manages indexing - no cache needed!")
    print("- First query might be slower (building connections)")
    print("- Subsequent queries will be faster with cached index")
    print("- Type 'quit' to exit")
    
    print("\n" + "=" * 50)
    
    # Interactive query
    while True:
        try:
            query = input("\nğŸ’¬ Enter your question (or 'quit' to exit): ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ Goodbye!")
                break
            
            if not query:
                continue
            
            # Thá»±c hiá»‡n truy váº¥n
            response = query_vector_database(query)
            
            print("\n" + "-" * 50)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Goodbye!")
            break
